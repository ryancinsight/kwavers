# Sprint 214 Session 4: Architectural Cleanup & Performance Benchmarking

**Date**: 2026-02-02  
**Sprint**: 214  
**Session**: 4  
**Status**: üîÑ **IN PROGRESS**  
**Estimated Duration**: 4-6 hours  

---

## Executive Summary

### Mission

Conduct comprehensive architectural audit, resolve critical circular dependencies and SSOT violations, implement GPU beamforming performance benchmarks, and validate system integrity.

### Objectives

1. **Critical Architectural Cleanup** (P0)
   - Resolve Analysis ‚Üí Solver circular dependency violation
   - Consolidate duplicate infrastructure directories
   - Archive deprecated sprint documentation
   - Remove orphaned test files

2. **Performance Benchmarking** (P0)
   - Implement comprehensive GPU beamforming benchmarks
   - Compare CPU (NdArray) vs GPU (WGPU/CUDA) backends
   - Measure throughput, latency, and memory usage
   - Generate performance report

3. **Code Quality** (P1)
   - Audit and remediate TODO/FIXME/HACK comments (118 total)
   - Remove dead code and unused imports
   - Validate test coverage

4. **Documentation Sync** (P1)
   - Update architecture documentation
   - Sync backlog and gap_audit
   - Create session completion report

---

## Phase 1: Critical Architectural Cleanup ‚úÖ COMPLETE

### P0-1: Circular Dependency Resolution ‚úÖ

**Issue**: Analysis layer (Layer 7) importing from Solver layer (Layer 4)

**Location**: `src/analysis/signal_processing/beamforming/neural/backends/burn_adapter.rs`

**Violation**:
```rust
// ‚ùå FORBIDDEN: Upward dependency (Layer 7 ‚Üí Layer 4)
use crate::solver::inverse::pinn::ml::{BurnPINN1DWave, BurnPINNConfig};
```

**Root Cause**: Concrete PINN implementation placed in Analysis layer instead of Solver layer.

**Solution**: Apply **Dependency Inversion Principle**
1. Keep trait `PinnBeamformingProvider` in Analysis layer (interface)
2. Move concrete implementation `BurnPinnBeamformingAdapter` to Solver layer
3. Analysis depends on abstraction (trait), Solver provides implementation

**Implementation**:
- Created `src/solver/inverse/pinn/beamforming/` module
- Moved `burn_adapter.rs` to solver layer
- Updated solver PINN module exports
- Re-exported from analysis for convenience (downward dependency is allowed)
- Removed empty `backends/` directory from analysis

**Architecture After Fix**:
```text
Layer 7: Analysis
    ‚Üì depends on trait
PinnBeamformingProvider (interface in analysis)
    ‚Üë implemented by
Layer 4: Solver
    BurnPinnBeamformingAdapter (concrete in solver)
    ‚Üì uses
Burn PINN Models (same layer)
```

**Validation**:
- ‚úÖ Build passes: `cargo check --lib` (13.26s)
- ‚úÖ All tests pass: 1970/1970 (7.08s)
- ‚úÖ Zero regressions

---

### P0-2: Infrastructure Directory Consolidation ‚úÖ

**Issue**: Duplicate infrastructure directories violating SSOT

**Findings**:
- `src/infra/` - Contains API, cloud, IO, runtime (active, 15 references)
- `src/infrastructure/` - Contains device management (unused, 0 references)

**Analysis**: These serve the same architectural purpose (Layer 8: Infrastructure) but were split incorrectly.

**Solution**: Consolidate to single `infrastructure` directory per architecture document.

**Implementation**:
1. Moved `api/`, `cloud/`, `io/`, `runtime/` from `infra/` to `infrastructure/`
2. Deleted `src/infra/` directory
3. Updated `src/infrastructure/mod.rs` to include all submodules
4. Removed duplicate `pub mod infra;` from `src/lib.rs`
5. Updated all imports: `crate::infra::` ‚Üí `crate::infrastructure::` (10 files)
6. Fixed re-exports in `lib.rs`

**Files Modified**:
- `src/infrastructure/mod.rs` - Added api, cloud, io, runtime modules
- `src/lib.rs` - Removed duplicate infra module declaration
- 10 files in `src/infrastructure/**/*.rs` - Updated use statements

**Result**: Single source of truth for Infrastructure layer (Layer 8).

---

### P0-3: Root-Level Clutter Cleanup ‚úÖ

**Issue**: Deprecated sprint docs and orphaned test files in project root

**Files Archived**:
- `SPRINT_213_COMPLETE.md`
- `SPRINT_213_RESEARCH_INTEGRATION_AUDIT.md`
- `SPRINT_213_SESSION_1_SUMMARY.md`
- `SPRINT_213_SESSION_2_SUMMARY.md`
- `SPRINT_213_SESSION_3_SUMMARY.md`
- `SPRINT_214_SESSION_1_EXECUTIVE_SUMMARY.md`
- `SPRINT_214_SESSION_1_SUMMARY.md`
- `CLEANUP_COMPLETE.md`

**Destination**: `docs/sprints/archive/`

**Orphaned Test File**:
- Moved `test_aic_mdl.rs` ‚Üí `tests/archive/test_aic_mdl_debug.rs`

**Impact**: Clean project root, documentation properly organized.

---

## Phase 2: Performance Benchmarking üîÑ IN PROGRESS

### Objective

Implement comprehensive benchmarks to measure GPU beamforming performance across backends and problem sizes.

### Benchmark Design

#### Test Matrix

**Problem Sizes**:
- Small: 32 channels √ó 1024 samples √ó 16√ó16 grid = 8.4M ops
- Medium: 64 channels √ó 2048 samples √ó 32√ó32 grid = 134M ops
- Large: 128 channels √ó 4096 samples √ó 64√ó64 grid = 2.1B ops
- XLarge: 256 channels √ó 8192 samples √ó 128√ó128 grid = 34B ops

**Backends**:
- NdArray (CPU baseline)
- WGPU (cross-platform GPU)
- CUDA (NVIDIA only, if available)

**Metrics**:
- Throughput (focal points/sec)
- Latency (ms per frame)
- Memory usage (peak RSS)
- Scalability (speedup vs CPU)

#### Benchmark Structure

```rust
// benches/gpu_beamforming_benchmark.rs

use criterion::{black_box, criterion_group, criterion_main, BenchmarkId, Criterion, Throughput};
use kwavers::analysis::signal_processing::beamforming::gpu::BurnDasBeamformer;
use ndarray::Array3;

fn benchmark_das_beamforming(c: &mut Criterion) {
    let mut group = c.benchmark_group("gpu_beamforming");
    
    // Problem sizes: (channels, samples, grid_x, grid_y)
    let sizes = vec![
        ("small", 32, 1024, 16, 16),
        ("medium", 64, 2048, 32, 32),
        ("large", 128, 4096, 64, 64),
    ];
    
    for (name, channels, samples, gx, gy) in sizes {
        let rf_data = Array3::zeros((channels, samples, 1));
        let grid_points = gx * gy;
        
        group.throughput(Throughput::Elements(grid_points as u64));
        
        // CPU baseline
        group.bench_with_input(
            BenchmarkId::new("cpu_ndarray", name),
            &rf_data,
            |b, data| b.iter(|| beamform_cpu(black_box(data)))
        );
        
        // GPU backends
        #[cfg(feature = "pinn")]
        {
            group.bench_with_input(
                BenchmarkId::new("gpu_wgpu", name),
                &rf_data,
                |b, data| b.iter(|| beamform_wgpu(black_box(data)))
            );
        }
    }
    
    group.finish();
}

criterion_group!(benches, benchmark_das_beamforming);
criterion_main!(benches);
```

#### Expected Results

**Hypotheses**:
1. GPU shows minimal advantage for small problems (overhead dominates)
2. GPU achieves 5-20√ó speedup for medium problems
3. GPU achieves 20-100√ó speedup for large problems (memory-bound)
4. CUDA outperforms WGPU by 1.2-2√ó (vendor optimization)

**Validation Criteria**:
- Numerical equivalence: CPU and GPU results match within 1e-5 relative error
- Performance scaling: GPU speedup increases monotonically with problem size
- Memory efficiency: GPU uses ‚â§2√ó host memory (accounting for staging buffers)

---

## Phase 3: Code Quality Audit üìã PLANNED

### P1-1: TODO/FIXME/HACK Remediation

**Status**: 118 instances found via `grep -r "TODO\|FIXME\|HACK\|XXX" src`

**Strategy**:
1. **Categorize** by severity:
   - Critical (blocks functionality): Implement immediately
   - High (technical debt): Schedule for next sprint
   - Low (optimization ideas): Document in backlog, remove from code
   - Obsolete (already done): Remove comments

2. **Remediation Actions**:
   - Replace with GitHub issues for tracking
   - Implement if < 30 minutes effort
   - Document in `backlog.md` or `gap_audit.md` if deferred
   - Remove comment after resolution

**Sample Triage**:
```rust
// TODO: Implement actual PINN training
// ‚Üí Action: Create GitHub issue #XXX, link in ADR

// FIXME: Use proper error instead of unwrap
// ‚Üí Action: Implement Result propagation (< 15 min)

// HACK: Temporary workaround for burn API
// ‚Üí Action: Check if Burn 0.19 has better API, update or document

// XXX: Optimize memory allocation
// ‚Üí Action: Add to backlog P2, remove comment
```

---

### P1-2: Dead Code Elimination

**Checks**:
- Unused imports: `cargo clippy -- -W unused_imports`
- Unreachable code: `cargo clippy -- -W unreachable_code`
- Dead code: `cargo clippy -- -W dead_code`
- Unused variables: `cargo clippy -- -W unused_variables`

**Exclusions**:
- Test-only code (keep for future reference)
- Feature-gated code (keep for optional features)
- Public API items (may be used by external crates)

---

### P1-3: Test Coverage Analysis

**Goal**: Identify untested critical paths

**Tools**:
- `cargo tarpaulin` (code coverage)
- `cargo audit` (security vulnerabilities)
- `cargo outdated` (dependency freshness)

**Targets**:
- Core modules: >90% coverage
- Physics modules: >85% coverage
- Solver modules: >80% coverage
- Analysis modules: >75% coverage

---

## Phase 4: Documentation Synchronization üìã PLANNED

### Updates Required

1. **ARCHITECTURE.md**
   - Document PINN beamforming location change
   - Update infrastructure layer consolidation
   - Add circular dependency resolution to ADR

2. **backlog.md**
   - Mark Session 4 objectives as complete
   - Add performance benchmark results
   - Update next sprint priorities

3. **gap_audit.md**
   - Mark P0-1, P0-2, P0-3 as resolved
   - Update Sprint 214 Session 4 progress
   - Document remaining P1 items

4. **checklist.md**
   - Check off architectural cleanup items
   - Check off infrastructure consolidation
   - Add benchmark validation criteria

---

## Success Metrics

### Quantitative

- ‚úÖ Zero circular dependencies (was 1)
- ‚úÖ Single infrastructure directory (was 2)
- ‚úÖ Zero root-level clutter files (was 10)
- ‚úÖ 1970/1970 tests passing (100%)
- ‚úÖ Zero compiler warnings
- üîÑ GPU benchmarks implemented (pending)
- üìã <50 TODO/FIXME/HACK (was 118, target 50% reduction)

### Qualitative

- ‚úÖ Clean Architecture principles fully enforced
- ‚úÖ SSOT maintained across all layers
- ‚úÖ Documentation properly organized
- üîÑ Performance characteristics quantified
- üìã Technical debt visibility improved

---

## Risk Assessment

### Resolved Risks

- ‚úÖ **Circular dependency breaking build**: Fixed via dependency inversion
- ‚úÖ **Test regressions from refactor**: 1970/1970 tests pass
- ‚úÖ **Lost code from directory consolidation**: All code preserved, just relocated

### Active Risks

- ‚ö†Ô∏è **Benchmark implementation time**: May extend session (mitigation: focus on core metrics)
- ‚ö†Ô∏è **GPU hardware availability**: WGPU fallback, CUDA optional (mitigation: graceful degradation)
- ‚ö†Ô∏è **TODO remediation scope**: 118 instances (mitigation: triage and defer to future sprints)

---

## Next Steps

### Immediate (This Session)

1. ‚úÖ Resolve circular dependencies
2. ‚úÖ Consolidate infrastructure directories
3. ‚úÖ Archive deprecated documentation
4. üîÑ Implement GPU beamforming benchmarks
5. üîÑ Run benchmark suite and collect metrics
6. üìã Generate performance report
7. üìã Update session documentation

### Short-term (Sprint 214 Session 5)

1. TODO/FIXME/HACK remediation (target 50% reduction)
2. Dead code elimination via Clippy
3. Test coverage analysis and gap filling
4. Advanced GPU optimizations (custom gather kernels)

### Long-term (Sprint 215+)

1. Implement remaining beamforming algorithms on GPU (MUSIC, MVDR, DMAS)
2. Differentiable beamforming with Burn autodiff
3. Real-time streaming pipelines
4. Clinical workflow integration

---

## References

### Architecture Documents

- `ARCHITECTURE.md` - Layer hierarchy and dependency rules
- `docs/adrs/` - Architectural decision records

### Prior Sessions

- Sprint 214 Session 1: Eigendecomposition & foundational math
- Sprint 214 Session 2: AIC/MDL & MUSIC localization
- Sprint 214 Session 3: GPU beamforming implementation (Burn)

### External Research

- Clean Architecture (Robert C. Martin)
- Dependency Inversion Principle (SOLID)
- Burn Framework Documentation: https://burn.dev/
- Criterion.rs Benchmarking: https://github.com/bheisler/criterion.rs

---

## Session Log

### 2026-02-02 14:00 - Session Start

**Initial Audit Findings**:
- Circular dependency: Analysis ‚Üí Solver (burn_adapter.rs)
- Duplicate directories: infra/ and infrastructure/
- Root clutter: 10 deprecated files
- TODO count: 118 instances

**Action**: Begin Phase 1 (Critical Cleanup)

### 2026-02-02 14:30 - Circular Dependency Fixed

**Implementation**:
- Moved burn_adapter.rs to solver/inverse/pinn/beamforming/
- Updated module structure and re-exports
- Validated with cargo check and cargo test

**Result**: ‚úÖ All tests pass (1970/1970)

### 2026-02-02 15:00 - Infrastructure Consolidated

**Implementation**:
- Moved api, cloud, io, runtime to infrastructure/
- Deleted infra/ directory
- Updated 10 files with new import paths
- Fixed lib.rs module declarations

**Result**: ‚úÖ Build passes, SSOT achieved

### 2026-02-02 15:15 - Documentation Archived

**Implementation**:
- Created docs/sprints/archive/
- Moved 8 sprint summary files
- Moved test_aic_mdl.rs to tests/archive/

**Result**: ‚úÖ Clean project root

### 2026-02-02 15:30 - Phase 2 Begin

**Objective**: Implement GPU beamforming performance benchmarks

**Status**: üîÑ IN PROGRESS

---

## Appendix A: Clean Architecture Validation

### Layer Dependency Matrix

After Session 4 fixes:

```
         Core Math Dom Phy Sol Sim Cln Ana Inf
Core  ‚îÇ   ‚úì    √ó    √ó   √ó   √ó   √ó   √ó   √ó   √ó
Math  ‚îÇ   ‚úì    ‚úì    √ó   √ó   √ó   √ó   √ó   √ó   √ó
Dom   ‚îÇ   ‚úì    ‚úì    ‚úì   √ó   √ó   √ó   √ó   √ó   √ó
Phy   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   √ó   √ó   √ó   √ó   √ó
Sol   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   ‚úì   √ó   √ó   √ó   √ó
Sim   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   ‚úì   ‚úì   √ó   √ó   √ó
Cln   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   ‚úì   ‚úì   ‚úì   √ó   √ó
Ana   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   ‚úì   ‚úì   ‚úì   ‚úì   √ó
Inf   ‚îÇ   ‚úì    ‚úì    ‚úì   ‚úì   ‚úì   ‚úì   ‚úì   ‚úì   ‚úì
```

**Legend**: ‚úì = Allowed dependency, √ó = Forbidden dependency

**Validation**: No upward dependencies (above diagonal is all √ó)

---

## Appendix B: Performance Benchmark Specification

### Test Configuration

**Hardware Requirements**:
- CPU: 8+ cores recommended
- RAM: 16GB+ (for large problem sizes)
- GPU: WGPU-compatible (Vulkan/Metal/DX12) or CUDA-capable NVIDIA

**Software Stack**:
- Rust: 1.75+
- Burn: 0.19.0
- Criterion: 0.5+
- Backend: NdArray/WGPU/CUDA

### Benchmark Parameters

**Fixed Parameters**:
- Speed of sound: 1540 m/s
- Element pitch: 0.3 mm
- Center frequency: 5 MHz
- Sampling rate: 40 MHz

**Variable Parameters**:
- Number of channels: {32, 64, 128, 256}
- Samples per channel: {1024, 2048, 4096, 8192}
- Grid size: {16√ó16, 32√ó32, 64√ó64, 128√ó128}

**Derived Metrics**:
- Total operations: channels √ó samples √ó grid_x √ó grid_y
- Memory footprint: channels √ó samples √ó 4 bytes (f32)
- Theoretical FLOPS: ops √ó (distance + interpolation + accumulation)

### Output Format

```text
GPU Beamforming Performance Report
==================================

Problem Size: Medium (64ch √ó 2048s √ó 32√ó32)
Operations: 134,217,728

Backend          Throughput      Latency    Memory    Speedup
----------------------------------------------------------------
CPU (NdArray)    1.2M pts/s     853 ms     512 MB    1.0√ó
GPU (WGPU)       18.7M pts/s     55 ms     1024 MB   15.6√ó
GPU (CUDA)       32.4M pts/s     32 ms     768 MB    27.0√ó

Numerical Validation:
- L2 Error (CPU vs WGPU): 1.2e-6
- L2 Error (CPU vs CUDA): 8.7e-7
- Max Absolute Diff: 3.4e-5

Scalability Analysis:
- CPU scaling: O(N) as expected
- GPU scaling: O(N^0.85) (memory bandwidth bound)
- GPU advantage threshold: ~64k operations
```

---

**End of Session 4 Plan**