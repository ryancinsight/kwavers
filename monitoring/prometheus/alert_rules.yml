groups:
  - name: pinn-api-alerts
    rules:
      - alert: PinnApiDown
        expr: up{job="pinn-api"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "PINN API is down"
          description: "PINN API has been down for more than 5 minutes."

      - alert: PinnApiHighErrorRate
        expr: rate(http_requests_total{job="pinn-api", status=~"5.."}[5m]) / rate(http_requests_total{job="pinn-api"}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate on PINN API"
          description: "PINN API error rate is {{ $value | printf \"%.2f\" }}% for the last 5 minutes."

      - alert: PinnApiHighLatency
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="pinn-api"}[5m])) > 1.0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency on PINN API"
          description: "PINN API 95th percentile latency is {{ $value | printf \"%.2f\" }}s for the last 5 minutes."

      - alert: PinnTrainingJobFailed
        expr: increase(pinn_training_jobs_failed_total[5m]) > 0
        labels:
          severity: warning
        annotations:
          summary: "Training job failed"
          description: "{{ $value | printf \"%.0f\" }} training jobs failed in the last 5 minutes."

  - name: kubernetes-alerts
    rules:
      - alert: KubernetesPodCrashLooping
        expr: increase(kube_pod_container_status_restarts_total{namespace="default", pod=~"pinn-api-.*"}[10m]) > 3
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "Pod crash looping"
          description: "Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} has restarted {{ $value | printf \"%.0f\" }} times in the last 10 minutes."

      - alert: KubernetesNodeOutOfMemory
        expr: (1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Node out of memory"
          description: "Node {{ $labels.instance }} memory usage is {{ $value | printf \"%.2f\" }}%."

  - name: gpu-alerts
    rules:
      - alert: GpuHighUtilization
        expr: DCGM_FI_DEV_GPU_UTIL > 95
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "GPU high utilization"
          description: "GPU {{ $labels.gpu }} utilization is {{ $value | printf \"%.0f\" }}%."

      - alert: GpuMemoryHighUsage
        expr: DCGM_FI_DEV_FB_USED / DCGM_FI_DEV_FB_TOTAL > 0.95
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "GPU memory high usage"
          description: "GPU {{ $labels.gpu }} memory usage is {{ $value | printf \"%.2f\" }}%."

  - name: database-alerts
    rules:
      - alert: DatabaseHighConnections
        expr: pg_stat_activity_count{datname="pinn_db"} > 50
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "Database has {{ $value | printf \"%.0f\" }} active connections."

      - alert: DatabaseDown
        expr: pg_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Database is down"
          description: "PostgreSQL database is not responding."

  - name: redis-alerts
    rules:
      - alert: RedisDown
        expr: redis_up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis cache is not responding."

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is {{ $value | printf \"%.2f\" }}%."
